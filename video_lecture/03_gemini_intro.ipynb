{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8301bd70",
   "metadata": {},
   "source": [
    "# Gemini intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94665a",
   "metadata": {},
   "source": [
    "The Gemini API can be used to generate many different outputs such as text, images, video and audio. In this tutorial, we'll go through text generation, check some metadata and tokens. In the end we'll explore multimodal inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94877dc5",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10080d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, here are 5 funny jokes about data engineering, in Markdown format:\n",
      "\n",
      "*   Why did the data engineer break up with the database? Because they couldn't commit! They needed more isolation, and less of a \"dirty read.\"\n",
      "\n",
      "*   What's a data engineer's favorite band? Metallica... because they're always trying to \"seek and destroy\" bad data!\n",
      "\n",
      "*   A data engineer walks into a bar and orders a beer. Then asks, \"Can you give me the schema of your tap list, the lineage of your ingredients, and is your supply chain immutable?\" The bartender replies, \"Get out.\"\n",
      "\n",
      "*   Why did the data engineer cross the road? To get to the other database... but needed to build a whole ETL pipeline just for that.\n",
      "\n",
      "*   My therapist asked me why I'm so obsessed with data modeling. I said, \"I'm just trying to normalize my life.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.0-flash\",\n",
    "    contents = \"Generate some funny jokes about data engineering. Give 5 points in markdown format\",\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "332ceafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4036e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      avg_logprobs=-0.5846955623532751,\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text=\"\"\"Okay, here are 5 funny jokes about data engineering in markdown format:\n",
       "\n",
       "*   Why did the data engineer break up with the database administrator? Because they couldn't see eye-to-eye on normalization. It was a non-relational relationship.\n",
       "\n",
       "*   What's a data engineer's favorite Halloween costume? A Data Lake Monster... it's big, messy, and everyone's afraid to go near it.\n",
       "\n",
       "*   Parallel processing is so good, I tried to parallel process my taxes. Now the IRS is giving me a segmentation fault.\n",
       "\n",
       "*   A data engineer walks into a bar and orders a beer. They then ask for a second beer. Then a third. Then a fourth. The bartender asks, \"Hey, are you going to order another?\" The data engineer replies, \"I'm just testing the latency.\"\n",
       "\n",
       "*   Why did the data engineer refuse to go to therapy? Because they said they were already working on their ETL issues.\n",
       "\"\"\"\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.0-flash',\n",
       "  response_id='e00cafz3A9jXvdIP98TqkQk',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=203,\n",
       "    candidates_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=203\n",
       "      ),\n",
       "    ],\n",
       "    prompt_token_count=15,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=15\n",
       "      ),\n",
       "    ],\n",
       "    total_token_count=218\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_gemini(prompt, model = \"gemini-2.0-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model = model,\n",
    "        contents = prompt,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "response = ask_gemini(prompt=\"Generate some funny jokes about data engineering. Give 5 points in markdown format\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2b119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "#knows that generateContentResponse is a pydantic model\n",
    "# -> we can work with in a OOP manner.\n",
    "isinstance(response, BaseModel)\n",
    "#uses this to explore the objects recieved from the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ecdea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44721e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.0-flash'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46421b",
   "metadata": {},
   "source": [
    "## Analyze tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31d24a",
   "metadata": {},
   "source": [
    "Tokens are the basic units of text for LLMs used to process and generate language. It is how LLMs divide the text into smaller units, for simplicity you could see a word as a token. Tokens are also what you pay for when you use the APIs\n",
    "\n",
    "The free tier in gemini API allows for (Gemini 2.5 flash)\n",
    "\n",
    "Price as of 2025-11-11\n",
    "Requests per minute (RPM): 10\n",
    "Tokens per minute (TPM): 250 000\n",
    "Requests per day (RDP): 250\n",
    "\n",
    "There is a possiblity to upgrade to higher tiers, which allows for more generous rate limits, but comes with higher costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a080435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=186,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=186\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=201\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea109b23",
   "metadata": {},
   "source": [
    "## System instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29784437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      avg_logprobs=-0.11464369181494356,\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text=\"\"\"Okay, let's break down Object-Oriented Programming (OOP) and dunder methods in Python, focusing on clarity and best practices.\n",
       "\n",
       "**Object-Oriented Programming (OOP)**\n",
       "\n",
       "OOP is a programming paradigm centered around \"objects.\" Think of objects as bundles of data (attributes) and functions (methods) that operate on that data.  It's a way to structure your code to mirror real-world entities and their interactions.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "*   **Class:** A blueprint or template for creating objects.  It defines the attributes and methods that objects of that class will have.\n",
       "\n",
       "*   **Object (Instance):**  A specific realization of a class.  You create objects from classes.\n",
       "\n",
       "*   **Encapsulation:** Bundling data and methods that operate on that data within a class.  It helps protect data and control access.\n",
       "\n",
       "*   **Abstraction:**  Hiding complex implementation details and exposing only essential information to the user.  Think of a car: you use the steering wheel and pedals without needing to know the inner workings of the engine.\n",
       "\n",
       "*   **Inheritance:**  Creating new classes (child classes) based on existing classes (parent classes).  The child class inherits attributes and methods from the parent, promoting code reuse and a hierarchical structure.\n",
       "\n",
       "*   **Polymorphism:**  The ability of objects of different classes to respond to the same method call in their own way. \"Many forms.\"\n",
       "\n",
       "**Example (Python):**\n",
       "\n",
       "```python\n",
       "class Dog:  # Class definition\n",
       "    def __init__(self, name, breed):  # Constructor (dunder method)\n",
       "        self.name = name  # Attribute\n",
       "        self.breed = breed  # Attribute\n",
       "\n",
       "    def bark(self):  # Method\n",
       "        print(\"Woof!\")\n",
       "\n",
       "my_dog = Dog(\"Buddy\", \"Golden Retriever\")  # Object (instance)\n",
       "print(my_dog.name)  # Accessing attribute\n",
       "my_dog.bark()  # Calling method\n",
       "```\n",
       "\n",
       "**Dunder Methods (Magic Methods)**\n",
       "\n",
       "Dunder methods (also called magic methods) are special methods in Python that start and end with double underscores (e.g., `__init__`, `__str__`, `__add__`). They provide a way to define how your objects behave with built-in Python operations and functions.\n",
       "\n",
       "**Key Points:**\n",
       "\n",
       "*   They allow you to overload operators (like `+`, `-`, `*`, `/`) and built-in functions (like `len()`, `str()`, `repr()`) for your custom classes.\n",
       "\n",
       "*   They are *not* meant to be called directly (although you *can*). Python calls them implicitly when you use the corresponding operator or function.\n",
       "\n",
       "**Common Dunder Methods:**\n",
       "\n",
       "*   `__init__(self, ...)`:  The constructor.  Called when an object is created.  Initializes the object's attributes.\n",
       "\n",
       "*   `__str__(self)`: Returns a user-friendly string representation of the object.  Used by `str()` and `print()`.\n",
       "\n",
       "*   `__repr__(self)`: Returns a string representation of the object that is ideally unambiguous and can be used to recreate the object.  Used by `repr()`.  If `__str__` is not defined, Python will use `__repr__`.  Good practice:  If you only define one, define `__repr__`.\n",
       "\n",
       "*   `__len__(self)`:  Returns the \"length\" of the object (e.g., number of items in a collection).  Used by `len()`.\n",
       "\n",
       "*   `__add__(self, other)`: Defines the behavior of the `+` operator.\n",
       "\n",
       "*   `__mul__(self, other)`: Defines the behavior of the `*` operator.\n",
       "\n",
       "*   `__eq__(self, other)`: Defines the behavior of the `==` operator (equality).  Important for comparisons.\n",
       "\n",
       "*   `__hash__(self)`:  Returns a hash value for the object.  Important if you want to use your objects as keys in dictionaries or in sets. If you implement `__eq__`, you *must* implement `__hash__`.\n",
       "\n",
       "**Example (Dunder Methods):**\n",
       "\n",
       "```python\n",
       "class Point:\n",
       "    def __init__(self, x, y):\n",
       "        self.x = x\n",
       "        self.y = y\n",
       "\n",
       "    def __str__(self):\n",
       "        return f\"Point({self.x}, {self.y})\"\n",
       "\n",
       "    def __add__(self, other):\n",
       "        return Point(self.x + other.x, self.y + other.y)\n",
       "\n",
       "p1 = Point(1, 2)\n",
       "p2 = Point(3, 4)\n",
       "\n",
       "print(p1)  # Output: Point(1, 2)  (Uses __str__)\n",
       "p3 = p1 + p2  # Uses __add__\n",
       "print(p3)  # Output: Point(4, 6)\n",
       "```\n",
       "\n",
       "**In Summary:**\n",
       "\n",
       "OOP provides a powerful way to structure code around objects, promoting reusability, maintainability, and a more intuitive representation of real-world problems. Dunder methods are the mechanism that allows your objects to seamlessly integrate with Python's built-in operators and functions, making your classes more expressive and Pythonic. They let you customize object behavior in fundamental ways.\n",
       "\"\"\"\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.0-flash',\n",
       "  response_id='O2Ycacu9K43qkdUPr8algQM',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=1135,\n",
       "    candidates_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=1135\n",
       "      ),\n",
       "    ],\n",
       "    prompt_token_count=61,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=61\n",
       "      ),\n",
       "    ],\n",
       "    total_token_count=1196\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.genai import types\n",
    "system_instruction= \"\"\"you are an expert in Python programming. You always provide idomatic code i.e. pythonic code. \n",
    "So when you see my code or my question. \n",
    "Be very critical but answer in a concise way. \n",
    "Also be constructive to help me improve\"\"\"\n",
    "prompt = \"Explain OOP and dunder methods\"\n",
    "\n",
    "def ask_gemini(prompt, model = \"gemini-2.0-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model = model,\n",
    "        contents = prompt,\n",
    "        config=types.GenerateContentConfig(system_instruction= system_instruction)\n",
    "    )\n",
    "    return response\n",
    "\n",
    "response = ask_gemini(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1992f165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=1135,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=1135\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=61,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=61\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=1196\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4ff371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135\n",
      "61\n",
      "1196\n"
     ]
    }
   ],
   "source": [
    "print(f\"{metadata.candidates_token_count}\")\n",
    "print(f\"{metadata.prompt_token_count}\")\n",
    "print(f\"{metadata.total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097dfc68",
   "metadata": {},
   "source": [
    "## Multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0596bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a furry, gray rabbit wearing a small, white graduation cap with a black brim. A yellow and blue ribbon drapes around the rabbit's neck, seemingly attached to the cap. The rabbit is sitting on a gray carpet.\n"
     ]
    }
   ],
   "source": [
    "text_input = \"describe this image shortly\"\n",
    "image_input= {\"mime_type\": \"image/png\", \"data\": open(\"bella.png\", 'rb').read()}\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=dict(\n",
    "        parts=[dict(text= text_input), dict(inline_data=image_input)]\n",
    "    )\n",
    ")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_platform_ai_course (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
